{
  "process_name": "SAP SuccessFactors to SFTP Integration with Error Handling",
  "description": "This integration extracts employee data from SAP SuccessFactors, transforms it, and transfers it to an SFTP server with comprehensive error handling.",
  "endpoints": [
    {
      "method": "GET",
      "path": "/odata/v2/User",
      "purpose": "Extract employee data from SAP SuccessFactors, transform it to Kafka Avro format, and upload to SFTP server",
      "components": [
        {
          "type": "groovy_script",
          "name": "Validate_Employee_Data",
          "id": "validate_data",
          "config": {
            "script": "ValidateEmployeeData.groovy"
          }
        },
        {
          "type": "request_reply",
          "name": "Get_SuccessFactors_Employee_Data",
          "id": "successfactors_request",
          "config": {
            "endpoint_path": "/odata/v2/User",
            "address": "${SF_API_URL}/odata/v2/User"
          }
        },
        {
          "type": "request_reply",
          "name": "Upload_To_SFTP",
          "id": "sftp_upload",
          "config": {
            "endpoint_path": "${SFTP_HOST}:${SFTP_PORT}/${target_directory}",
            "address": "sftp://${SFTP_USER}:${SFTP_PASSWORD}@${SFTP_HOST}:${SFTP_PORT}/${target_directory}"
          }
        },
        {
          "type": "groovy_script",
          "name": "Transform_Canonical_To_Kafka_Avro",
          "id": "transform_to_kafka_avro",
          "config": {
            "script": "CanonicalToKafkaAvro.groovy"
          }
        }
      ],
      "error_handling": {
        "exception_subprocess": [
          {
            "type": "enricher",
            "name": "Prepare_Error_Message",
            "id": "error_message_enricher",
            "trigger": "any_error",
            "config": {
              "content": "{ \"error\": \"${exception.message}\", \"timestamp\": \"${date:now:yyyy-MM-dd'T'HH:mm:ss}\", \"process\": \"SAP SuccessFactors to SFTP Integration\" }"
            }
          },
          {
            "type": "groovy_script",
            "name": "Log_Error_Details",
            "id": "log_error",
            "trigger": "any_error",
            "config": {
              "script": "LogErrorDetails.groovy"
            }
          },
          {
            "type": "request_reply",
            "name": "Send_Error_Notification",
            "id": "send_notification",
            "trigger": "any_error",
            "config": {
              "endpoint_path": "/notification/email",
              "address": "smtp://${SMTP_HOST}:${SMTP_PORT}"
            }
          }
        ]
      },
      "branching": {
        "type": "exclusive",
        "branches": [
          {
            "condition": "Valid data",
            "components": [
              "validate_data",
              "transform_to_kafka_avro",
              "sftp_upload"
            ],
            "sequence": [
              "validate_data",
              "transform_to_kafka_avro",
              "sftp_upload"
            ]
          },
          {
            "condition": "Invalid data",
            "components": [
              "validate_data",
              "error_message_enricher",
              "log_error",
              "send_notification"
            ],
            "sequence": [
              "validate_data",
              "error_message_enricher",
              "log_error",
              "send_notification"
            ]
          }
        ]
      },
      "sequence": [
        "successfactors_request",
        "validate_data",
        "transform_to_kafka_avro",
        "sftp_upload"
      ],
      "transformations": [
        {
          "name": "CanonicalToKafkaAvro.groovy",
          "type": "groovy",
          "script": "import groovy.json.*\n\ndef inputBody = message.getBody(String.class)\ndef jsonSlurper = new JsonSlurper()\ndef data = jsonSlurper.parseText(inputBody)\n\ndef result = [:]\nresult.put('Object', [:])\n\n// Create batch processing directives\ndef batchProcessingDirectives = [:]\nbatchProcessingDirectives.put('Object', [:])\n\n// Map accountID/username from field 9\ndef accountID = [:]\naccountID.put('Object', [:])\naccountID.get('Object').put('username', data['9'])\nbatchProcessingDirectives.get('Object').put('accountID', accountID)\n\n// Map batch processing option from field 118\ndef batchProcessingOption = [:]\nbatchProcessingOption.put('Array', [])\ndef arrayElement1 = [:]\narrayElement1.put('Object', [:])\narrayElement1.get('Object').put('name', data['118'])\nbatchProcessingOption.get('Array').add(arrayElement1)\nbatchProcessingDirectives.get('Object').put('batchProcessingOption', batchProcessingOption)\n\n// Add batch processing directives to result\nresult.get('Object').put('batchProcessingDirectives', batchProcessingDirectives)\n\n// Create batch contact list\ndef batchContactList = [:]\nbatchContactList.put('Array', [])\ndef contactArrayElement1 = [:]\ncontactArrayElement1.put('Object', [:])\n\n// Create contact array\ndef contact = [:]\ncontact.put('Array', [])\ndef contactElement1 = [:]\ncontactElement1.put('Object', [:])\n\n// Map contactID from field 91\ncontactElement1.get('Object').put('contactID', data['91'])\n\n// Create contact point list\ndef contactPointList = [:]\ncontactPointList.put('Array', [])\ndef contactPointElement1 = [:]\ncontactPointElement1.put('Object', [:])\n\n// Create contact point\ndef contactPoint = [:]\ncontactPoint.put('Array', [])\ndef contactPointTypeElement = [:]\ncontactPointTypeElement.put('Object', [:])\n\n// Map type from field 111\ncontactPointTypeElement.get('Object').put('type', data['111'])\ncontactPoint.get('Array').add(contactPointTypeElement)\n\n// Build the nested structure\ncontactPointElement1.get('Object').put('contactPoint', contactPoint)\ncontactPointList.get('Array').add(contactPointElement1)\ncontactElement1.get('Object').put('contactPointList', contactPointList)\ncontact.get('Array').add(contactElement1)\ncontactArrayElement1.get('Object').put('contact', contact)\nbatchContactList.get('Array').add(contactArrayElement1)\n\n// Add batch contact list to result\nresult.get('Object').put('batchContactList', batchContactList)\n\n// Convert result to JSON\ndef jsonBuilder = new JsonBuilder(result)\nreturn jsonBuilder.toString()"
        },
        {
          "name": "ValidateEmployeeData.groovy",
          "type": "groovy",
          "script": "import groovy.json.*\n\ndef inputBody = message.getBody(String.class)\ndef jsonSlurper = new JsonSlurper()\ndef data\n\ntry {\n    data = jsonSlurper.parseText(inputBody)\n    \n    // Check for required fields\n    def requiredFields = ['9', '91', '111', '118']\n    def missingFields = []\n    \n    requiredFields.each { field ->\n        if (!data.containsKey(field) || data[field] == null || data[field].toString().trim() == '') {\n            missingFields.add(field)\n        }\n    }\n    \n    if (missingFields.size() > 0) {\n        throw new Exception(\"Missing required fields: ${missingFields.join(', ')}\")\n    }\n    \n    // Data is valid\n    return inputBody\n    \n} catch (Exception e) {\n    // Set property to indicate validation failure\n    message.setProperty(\"validation.error\", \"true\")\n    message.setProperty(\"error.message\", e.getMessage())\n    \n    // Return original message for error handling\n    return inputBody\n}"
        },
        {
          "name": "LogErrorDetails.groovy",
          "type": "groovy",
          "script": "import groovy.json.*\n\ndef inputBody = message.getBody(String.class)\ndef errorMessage = message.getProperty(\"error.message\")\ndef timestamp = new Date().format(\"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n\n// Create detailed log entry\ndef logEntry = [\n    timestamp: timestamp,\n    process: \"SAP SuccessFactors to SFTP Integration\",\n    error: errorMessage,\n    payload: inputBody\n]\n\n// Convert to JSON for logging\ndef jsonBuilder = new JsonBuilder(logEntry)\ndef logJson = jsonBuilder.toString()\n\n// Log the error details\nprintln \"ERROR: ${logJson}\"\n\n// Return the error details for notification\nreturn logJson"
        }
      ],
      "sequence_flows": [
        {
          "id": "SequenceFlow_Start",
          "source": "validate_data",
          "target": "successfactors_request",
          "is_immediate": true,
          "xml_content": "<bpmn2:sequenceFlow id=\"SequenceFlow_Start\" sourceRef=\"validate_data\" targetRef=\"successfactors_request\" isImmediate=\"true\"/>"
        },
        {
          "id": "SequenceFlow_1",
          "source": "successfactors_request",
          "target": "sftp_upload",
          "is_immediate": true,
          "xml_content": "<bpmn2:sequenceFlow id=\"SequenceFlow_1\" sourceRef=\"successfactors_request\" targetRef=\"sftp_upload\" isImmediate=\"true\"/>"
        },
        {
          "id": "SequenceFlow_End",
          "source": "sftp_upload",
          "target": "transform_to_kafka_avro",
          "is_immediate": true,
          "xml_content": "<bpmn2:sequenceFlow id=\"SequenceFlow_End\" sourceRef=\"sftp_upload\" targetRef=\"transform_to_kafka_avro\" isImmediate=\"true\"/>"
        }
      ]
    }
  ]
}